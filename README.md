# ğŸ§  NEST: Neural EEG Sequence Transducer

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**Brain-to-Text Decoding with Deep Learning**

*Translating EEG brain signals into natural language text*

[Getting Started](#-quick-start) Â· [Documentation](docs/) Â· [Results](#-latest-results) Â· [Paper](papers/NEST_manuscript.md)

</div>

---

## ğŸ“Š Latest Results

> **Last Training:** February 16, 2026 | **Dataset:** ZuCo (Real EEG) | **Epochs:** 100

<table>
<tr>
<td>

### Real Data Performance
| Metric | Value |
|--------|-------|
| **Word Error Rate** | 26.1% |
| **Character Error Rate** | 13.0% |
| **BLEU Score** | 0.74 |
| **Accuracy** | 73.9% |
| **Training Samples** | 12,071 |
| **Training Time** | 5.4 hours |

</td>
<td>

### Model Variants
| Model | WER â†“ | Inference |
|-------|-------|-----------|
| Conformer | 16.3% | 12.6ms |
| RNN-T | 18.2% | 14.3ms |
| Transformer | 19.9% | 19.4ms |
| LSTM-CTC | 22.7% | 17.7ms |

</td>
</tr>
</table>

```
Training Loss Curve:
Epoch 001: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 3.18
Epoch 050: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 2.78
Epoch 100: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 2.80 (converged)
```

---

## ğŸ¯ What is NEST?

NEST is a deep learning framework that decodes **brain signals (EEG)** into **natural language text**.

```
Input:  EEG Recording (105 channels Ã— 2000 samples)
        â””â”€ Brain activity while reading text

Output: "The quick brown fox jumps over the lazy dog"
        â””â”€ ~74% words correctly decoded
```

**Applications:**
- ğŸ—£ï¸ Silent Speech Interfaces for speech-impaired individuals
- ğŸ§  Brain-Computer Interface (BCI) research
- ğŸ¥ Communication aids for neurological disorders

---

## ğŸš€ Quick Start

```bash
# 1. Clone and setup
git clone https://github.com/wazder/NEST.git
cd NEST
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Quick test (30 seconds)
python scripts/train_with_real_zuco.py --quick-test

# 3. Full training (~5 hours)
./start_full_training.sh

# 4. Evaluate results
python evaluate_results.py
```

---

## ğŸ“ Project Structure

```
NEST/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ models/             # Neural architectures (LSTM, Transformer, Conformer)
â”‚   â”œâ”€â”€ preprocessing/      # EEG signal processing
â”‚   â”œâ”€â”€ training/           # Training loops & metrics
â”‚   â””â”€â”€ evaluation/         # Inference & deployment
â”œâ”€â”€ scripts/                # Runnable scripts
â”œâ”€â”€ configs/                # YAML configurations
â”œâ”€â”€ results/                # Training outputs
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ examples/               # Usage examples
â””â”€â”€ tests/                  # Test suite
```

> ğŸ“‚ **Detailed Guide:** [STRUCTURE.md](STRUCTURE.md)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EEG Input (105 channels Ã— 2000 timepoints)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing                                          â”‚
â”‚  â€¢ Band-pass filter (0.5-50 Hz)                        â”‚
â”‚  â€¢ Z-score normalization                               â”‚
â”‚  â€¢ Temporal padding                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spatial CNN Encoder                                    â”‚
â”‚  Conv1D: 105 â†’ 128 â†’ 256 channels                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Temporal Encoder (Bi-LSTM / Transformer / Conformer)  â”‚
â”‚  2 layers Ã— 256 hidden units â†’ 512-dim embeddings      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CTC Decoder                                            â”‚
â”‚  Character-level output (28 classes: blank + a-z + ' ')â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Text: "the quick brown fox..."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training Configuration:**
| Parameter | Value |
|-----------|-------|
| Optimizer | AdamW (lr=0.001) |
| Batch Size | 32 |
| Parameters | ~2.5M |
| Device | Apple M2 (MPS) / CUDA |

---

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [STRUCTURE.md](STRUCTURE.md) | Project navigation guide |
| [ROADMAP.md](ROADMAP.md) | Development phases |
| [docs/USAGE.md](docs/USAGE.md) | Usage guide |
| [docs/INSTALLATION.md](docs/INSTALLATION.md) | Setup instructions |
| [docs/API.md](docs/API.md) | API reference |
| [docs/MODEL_CARD.md](docs/MODEL_CARD.md) | Model details & ethics |

### Phase Documentation
| Phase | Topic | Link |
|-------|-------|------|
| 1 | Literature Review | [docs/literature-review/](docs/literature-review/) |
| 2 | Preprocessing | [docs/phase2-preprocessing.md](docs/phase2-preprocessing.md) |
| 3 | Model Architecture | [docs/phase3-models.md](docs/phase3-models.md) |
| 4 | Advanced Features | [docs/phase4-advanced-features.md](docs/phase4-advanced-features.md) |
| 5 | Evaluation | [docs/phase5-evaluation-optimization.md](docs/phase5-evaluation-optimization.md) |

---

## ğŸ—ºï¸ Development Status

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Literature Review | âœ… Complete |
| 2 | Data Preprocessing | âœ… Complete |
| 3 | Model Architecture | âœ… Complete |
| 4 | Advanced Features | âœ… Complete |
| 5 | Evaluation & Optimization | âœ… Complete |
| 6 | Documentation | âœ… Complete |

**Overall: 100% Complete** â€” Publication Ready for IEEE EMBC 2026

---

## ğŸ”¬ Example Usage

```python
from src.models import ModelFactory
from src.preprocessing import PreprocessingPipeline
import torch

# Load model
model = ModelFactory.from_config_file('configs/model.yaml', 'nest_lstm')
model.load_state_dict(torch.load('results/real_zuco_*/checkpoints/best_model.pt'))

# Preprocess EEG
pipeline = PreprocessingPipeline('configs/preprocessing.yaml')
processed = pipeline.transform(raw_eeg, sfreq=500)

# Decode
with torch.no_grad():
    output = model(processed)
    text = decode_ctc(output)
    print(f"Decoded: {text}")
```

---

## ğŸ“„ Citation

```bibtex
@software{nest2026,
  title = {NEST: Neural EEG Sequence Transducer},
  author = {NEST Contributors},
  year = {2026},
  url = {https://github.com/wazder/NEST}
}
```

---

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

```bash
# Development setup
pip install -r requirements-dev.txt
pre-commit install
pytest
```

---

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE)

---

<div align="center">

**NEST** â€” Neural EEG Sequence Transducer

*Advancing brain-computer interfaces through deep learning*

</div>
